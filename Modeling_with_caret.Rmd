---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

Now, import the library
```{r}
#load in libraries needed
library(tidyverse)
library(dplyr)
library(magrittr)
library(stats)
library(ggplot2)
library(broom)
library(car)
library(knitr)
library(caret)
library(RANN)
```
Then Import the data
```{r}
#load in the data
TV <- read.csv("data/Training_values.csv")
TL <- read.csv("data/Training_labels.csv")
TestValues <- read.csv("data/Test_values.csv")

#data preparation
TrainingData <- merge(TV, TL, by="row_id")#used in the stage of exploration
AllValues <- rbind(TV, TestValues)#used in the stage of prediction
```
Train a linear model, model0
```{r}
model0 <- train(x=TV, y=TL$poverty_rate, method="lm", trControl=trainControl(method="cv", number=5, verboseIter=TRUE))

model0
```

Train a linear model, model1, w/ medianImpute
```{r}
model1 <- train(x=TV, y=TL$poverty_rate, method="lm", preProcess="medianImpute", trControl=trainControl(method="cv", number=5, verboseIter=TRUE))

model1
```
Seems like NA is causing the error previouly.

Train a linear model, model2, w/ knnImpute
```{r}
model2 <- train(x=TV, y=TL$poverty_rate, method="lm", preProcess="knnImpute", trControl=trainControl(method="cv", number=5, verboseIter=TRUE))

model2
```
Better RMSE from knnImpute. And notice that "center" and "scale" are carried out automatically after knnImpute.

Train a linear model, model3, w/ knnImpute and pca
```{r}
model3 <- train(x=TV, y=TL$poverty_rate, method="lm", preProcess=c("knnImpute","pca"), trControl=trainControl(method="cv", number=5, verboseIter=TRUE))

model3
```
Not satisfactory result. Notice that again, "center" and "scale" are carried out automatically, but should be in between knnImpute and pca.

Train a linear model, model4, same as model3 but specifies order of preProcess.
```{r}
model4 <- train(x=TV, y=TL$poverty_rate, method="lm", preProcess=c("knnImpute","center","scale","pca"), trControl=trainControl(method="cv", number=5, verboseIter=TRUE))

model4
```
slightly better than model3, but clearly no improvement in performance with pca. Once again, high variance variables don't gaurantee better prediction!

<<<<<<< HEAD
There is flaw in early discussion that not a single cross validation plan is being made to the base of comparison. (i.e. I might have the result by chance.)
```{r}
set.seed(2018)

#Create custom indeces
myFolds <- createFolds(TL$poverty_rate, k=5)

#Then define a reusable trainControl object
myControl <- trainControl(method="cv", verboseIter=TRUE, savePredictions=TRUE, index=myFolds)
```

Now, let's try glmnet algorithm
```{r}
model_1 <- train(x=TV, y=TL$poverty_rate, method="glmnet", tuneGrid=expand.grid(alpha=0:1, lambda=seq(0.001, 0.1, length=10)), preProcess="knnImpute", trControl=myControl)

plot(model_1)
```
some say it's because categorical variables are not yet factor...
```{r}
#remove id
temp <- TV[,-1]
#make them factors
temp$area__rucc <- as.factor(temp$area__rucc)
temp$area__urban_influence <- as.factor(temp$area__urban_influence)
temp$econ__economic_typology <- as.factor(temp$econ__economic_typology)
temp$yr <- as.factor(temp$yr)

model_1 <- train(x=temp, y=TL$poverty_rate, method="glmnet", tuneGrid=expand.grid(alpha=0:1, lambda=seq(0.001, 0.1, length=10)), preProcess="medianImpute", trControl=myControl)

plot(model_1)
```
use model.matrix instead...
```{r}
temp <- data.frame(model.matrix(poverty_rate~.,TrainingData)[,-1])
model_1 <- train(x=temp, y=TL$poverty_rate, method="glmnet", tuneGrid=expand.grid(alpha=0:1, lambda=seq(0.001, 0.1, length=10)), preProcess="medianImpute", trControl=myControl)

plot(model_1)
```
use fmla form of caret train...
```{r}
model_1 <- train(poverty_rate~., data=TrainingData, method="glmnet", tuneGrid=expand.grid(alpha=0:1, lambda=seq(0.001, 0.1, length=10)), preProcess="medianImpute", trControl=myControl)

plot(model_1)
```
or maybe it's becuase some columns are too heavily missing
```{r}
#function used for finding missing percentage of feature concerned
pct_missing <- function(x){
  sum(is.na(x))/length(x)*100
}

temp <- TV[apply(TV, 1, pct_missing)<=10,]
temp <- temp[,apply(temp, 2, pct_missing)<=10]

model_1 <- train(x=temp, y=TL$poverty_rate, method="glmnet", tuneGrid=expand.grid(alpha=0:1, lambda=seq(0.001, 0.1, length=10)), preProcess="medianImpute", trControl=myControl)

plot(model_1)

```
Uh...maybe due to some zero-variance or near-zero-variance predictor...

```{r}
dim(TV)
nzv <- nearZeroVar(TV)
filteredTV <- TV[,-nzv]
dim(filteredTV)
```
does that mean every predictor is discarded due to near zero variance?
```{r}
nearZeroVar(TV, saveMetrics=TRUE)
```
I almost made a silly mistake.

try randome forest
```{r}
model_2 <- train(x=TV, y=TL$poverty_rate, method="ranger", tuneLength=10, trControl=myControl, preProcess="medianImpute")

plot(model_2)
```

try xgboost
```{r}
model_3 <- train(x=TV, y=TL$poverty_rate, method="xgbTree", trControl=myControl, preProcess="medianImpute")

plot(model_3)
```

