---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

Now, import the library
```{r}
#load in libraries needed
library(tidyverse)
library(dplyr)
library(magrittr)
library(stats)
library(ggplot2)
library(broom)
library(car)
library(knitr)
library(caret)
library(RANN)
```
Then Import the data
```{r}
#load in the data
TV <- read.csv("data/Training_values.csv")
TL <- read.csv("data/Training_labels.csv")
TestValues <- read.csv("data/Test_values.csv")

#data preparation
TrainingData <- merge(TV, TL, by="row_id")#used in the stage of exploration
AllValues <- rbind(TV, TestValues)#used in the stage of prediction
```
###Try once
####Train a linear model, model0
```{r}
model0 <- train(x=TV, y=TL$poverty_rate, method="lm", trControl=trainControl(method="cv", number=5, verboseIter=TRUE))

model0
```
There are missing values to be dealt with.

####Train a linear model, model1, w/ medianImpute
```{r}
model1 <- train(x=TV, y=TL$poverty_rate, method="lm", preProcess="medianImpute", trControl=trainControl(method="cv", number=5, verboseIter=TRUE))

model1
```
Seems like NA is causing the error previouly.

####Train a linear model, model2, w/ knnImpute
```{r}
model2 <- train(x=TV, y=TL$poverty_rate, method="lm", preProcess="knnImpute", trControl=trainControl(method="cv", number=5, verboseIter=TRUE))

model2
```
Better RMSE from knnImpute. And notice that "center" and "scale" are carried out automatically after knnImpute.

####Train a linear model, model3, w/ knnImpute and pca
```{r}
model3 <- train(x=TV, y=TL$poverty_rate, method="lm", preProcess=c("knnImpute","pca"), trControl=trainControl(method="cv", number=5, verboseIter=TRUE))

model3
```
Not satisfactory result. Notice that again, "center" and "scale" are carried out automatically, but should be in between knnImpute and pca.

####Train a linear model, model4, same as model3 but specifies order of preProcess.
```{r}
model4 <- train(x=TV, y=TL$poverty_rate, method="lm", preProcess=c("knnImpute","center","scale","pca"), trControl=trainControl(method="cv", number=5, verboseIter=TRUE))

model4
```
slightly better than model3, but clearly no improvement in performance with pca. Once again, high variance variables don't gaurantee better prediction!

###Try twice
There is flaw in early discussion that not a single cross validation plan is being made to the base of comparison. (i.e. I might have the result by chance.)
####myControl
Let's make up a single trainControl
```{r}
set.seed(2018)

#Create custom indeces
myFolds <- createFolds(TL$poverty_rate, k=5)

#Then define a reusable trainControl object
myControl <- trainControl(method="cv", verboseIter=TRUE, savePredictions=TRUE, index=myFolds)
```

####glmnet
Now, let's try glmnet algorithm
```{r}
model_1 <- train(x=TV, y=TL$poverty_rate, method="glmnet", tuneGrid=expand.grid(alpha=0:1, lambda=seq(0.001, 0.1, length=10)), preProcess="knnImpute", trControl=myControl)

```
some say it's because categorical variables are not yet factor...Main difference between function lm and glmnet is that the former is capable of dealing with categorical variables, but the latter isn't. So let's do it for glmnet.

####factorize categorial varialbes
```{r}
#remove id
temp <- TV[,-1]
#make them factors
temp$area__rucc <- as.factor(temp$area__rucc)
temp$area__urban_influence <- as.factor(temp$area__urban_influence)
temp$econ__economic_typology <- as.factor(temp$econ__economic_typology)
temp$yr <- as.factor(temp$yr)

model_1 <- train(x=temp, y=TL$poverty_rate, method="glmnet", tuneGrid=expand.grid(alpha=0:1, lambda=seq(0.001, 0.1, length=10)), preProcess="medianImpute", trControl=myControl)
```
Only to find out that categorical variables in TV are already factors...So a more percise solution is not factorization but dummification.

####model.matrix
use model.matrix to dummify those categorical variables.
```{r}
temp <- data.frame(model.matrix(poverty_rate~.,TrainingData)[,-1])
model_1 <- train(x=temp, y=TL$poverty_rate, method="glmnet", tuneGrid=expand.grid(alpha=0:1, lambda=seq(0.001, 0.1, length=10)), preProcess="medianImpute", trControl=myControl)

```
model.matrix automatically removed observations with NA, which leads to dimensions between temp and TL unmatched.

####dummyVars
use dummyVars instead (also from caret)
```{r}
temp <- data.frame(dummyVars(poverty_rate~.,TrainingData))
model_1 <- train(x=temp, y=TL$poverty_rate, method="glmnet", tuneGrid=expand.grid(alpha=0:1, lambda=seq(0.001, 0.1, length=10)), preProcess="medianImpute", trControl=myControl)
```
then why not put into train without coercing?
```{r}
temp <- dummyVars(poverty_rate~.,TrainingData)
model_1 <- train(x=temp, y=TL$poverty_rate, method="glmnet", tuneGrid=expand.grid(alpha=0:1, lambda=seq(0.001, 0.1, length=10)), preProcess="medianImpute", trControl=myControl)

```
Seems like data.frame is required, but how? I mean dummyVars is a completely bizarre object...Some say it's possible to use predict
```{r}
temp <- dummyVars(poverty_rate~.,TrainingData)
temp <- data.frame(predict(temp, newdata=TrainingData))
model_1 <- train(x=temp, y=TL$poverty_rate, method="glmnet", tuneGrid=expand.grid(alpha=0:1, lambda=seq(0.0001, 0.1, length=20)), preProcess="medianImpute", trControl=myControl)

plot(model_1)
```
Finally.
Lesson learned: glmnet only takes quatitative variables and make sure they are complete without missing values.
So one can see the different performances between ridge regression(0) and lasso regression(1). Let's include a 50% ridge and 50% lasso to play a little bit.
```{r}
temp <- dummyVars(poverty_rate~.,TrainingData)
temp <- data.frame(predict(temp, newdata=TrainingData))
model_1 <- train(x=temp, y=TL$poverty_rate, method="glmnet", tuneGrid=expand.grid(alpha=c(0,0.25,0.5,0.75,1), lambda=seq(0.0001, 0.1, length=20)), preProcess="medianImpute", trControl=myControl)

plot(model_1)
model_1
```
From calling model_1 on the console, I found the RMSE for alpha=0.5 and lambda=0.07896842, is 3.270547. Nowhere better than full model...

####ranger
try a randome forest
```{r}
model_2 <- train(x=TV, y=TL$poverty_rate, method="ranger", tuneLength=10, trControl=myControl, preProcess="medianImpute")

plot(model_2)
```
Resulting in RMSE=3.148711. Similar to the full linear model.

####xbgTree
try xgboost.
```{r}
model_3 <- train(x=TV, y=TL$poverty_rate, method="xgbTree", trControl=myControl, preProcess="medianImpute")

plot(model_3)
```

